{
  "description": "DataprocBatch is the Schema for the DataprocBatch API",
  "type": "object",
  "required": [
    "spec"
  ],
  "properties": {
    "apiVersion": {
      "description": "APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources",
      "type": "string"
    },
    "kind": {
      "description": "Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds",
      "type": "string"
    },
    "metadata": {
      "type": "object"
    },
    "spec": {
      "description": "DataprocBatchSpec defines the desired state of DataprocBatch",
      "type": "object",
      "properties": {
        "environmentConfig": {
          "description": "Optional. Environment configuration for the batch execution.",
          "type": "object",
          "properties": {
            "executionConfig": {
              "description": "Optional. Execution configuration for a workload.",
              "type": "object",
              "properties": {
                "idleTTL": {
                  "description": "Optional. Applies to sessions only. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 1 hour if not set. If both `ttl` and `idle_ttl` are specified for an interactive session, the conditions are treated as `OR` conditions: the workload will be terminated when it has been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs first.",
                  "type": "string"
                },
                "kmsKeyRef": {
                  "description": "Optional. The Cloud KMS key to use for encryption.",
                  "type": "object",
                  "oneOf": [
                    {
                      "required": [
                        "name"
                      ],
                      "not": {
                        "required": [
                          "external"
                        ]
                      }
                    },
                    {
                      "required": [
                        "external"
                      ],
                      "not": {
                        "anyOf": [
                          {
                            "required": [
                              "name"
                            ]
                          },
                          {
                            "required": [
                              "namespace"
                            ]
                          }
                        ]
                      }
                    }
                  ],
                  "properties": {
                    "external": {
                      "description": "A reference to an externally managed KMSCryptoKey. Should be in the format `projects/[kms_project_id]/locations/[region]/keyRings/[key_ring_id]/cryptoKeys/[key]`.",
                      "type": "string"
                    },
                    "name": {
                      "description": "The `name` of a `KMSCryptoKey` resource.",
                      "type": "string"
                    },
                    "namespace": {
                      "description": "The `namespace` of a `KMSCryptoKey` resource.",
                      "type": "string"
                    }
                  },
                  "additionalProperties": false
                },
                "networkTags": {
                  "description": "Optional. Tags used for network traffic control.",
                  "type": "array",
                  "items": {
                    "type": "string"
                  }
                },
                "networkURI": {
                  "description": "Optional. Network URI to connect workload to.",
                  "type": "string"
                },
                "serviceAccountRef": {
                  "description": "Optional. Service account that used to execute workload.",
                  "type": "object",
                  "oneOf": [
                    {
                      "required": [
                        "name"
                      ],
                      "not": {
                        "required": [
                          "external"
                        ]
                      }
                    },
                    {
                      "required": [
                        "external"
                      ],
                      "not": {
                        "anyOf": [
                          {
                            "required": [
                              "name"
                            ]
                          },
                          {
                            "required": [
                              "namespace"
                            ]
                          }
                        ]
                      }
                    }
                  ],
                  "properties": {
                    "external": {
                      "description": "The `email` field of an `IAMServiceAccount` resource.",
                      "type": "string"
                    },
                    "name": {
                      "description": "Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names",
                      "type": "string"
                    },
                    "namespace": {
                      "description": "Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/",
                      "type": "string"
                    }
                  },
                  "additionalProperties": false
                },
                "stagingBucketRef": {
                  "description": "Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. **This field requires a Cloud Storage bucket name, not a `gs://...` URI to a Cloud Storage bucket.**",
                  "type": "object",
                  "oneOf": [
                    {
                      "required": [
                        "name"
                      ],
                      "not": {
                        "required": [
                          "external"
                        ]
                      }
                    },
                    {
                      "required": [
                        "external"
                      ],
                      "not": {
                        "anyOf": [
                          {
                            "required": [
                              "name"
                            ]
                          },
                          {
                            "required": [
                              "namespace"
                            ]
                          }
                        ]
                      }
                    }
                  ],
                  "properties": {
                    "external": {
                      "description": "A reference to an externally-managed StorageBucket resource.",
                      "type": "string"
                    },
                    "name": {
                      "description": "The name of a StorageBucket resource.",
                      "type": "string"
                    },
                    "namespace": {
                      "description": "The namespace of a StorageBucket resource.",
                      "type": "string"
                    }
                  },
                  "additionalProperties": false
                },
                "subnetworkURI": {
                  "description": "Optional. Subnetwork URI to connect workload to.",
                  "type": "string"
                },
                "ttl": {
                  "description": "Optional. The duration after which the workload will be terminated, specified as the JSON representation for [Duration](https://protobuf.dev/programming-guides/proto3/#json). When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If `ttl` is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or run forever without exiting). If `ttl` is not specified for an interactive session, it defaults to 24 hours. If `ttl` is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours. Minimum value is 10 minutes; maximum value is 14 days. If both `ttl` and `idle_ttl` are specified (for an interactive session), the conditions are treated as `OR` conditions: the workload will be terminated when it has been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs first.",
                  "type": "string"
                }
              },
              "additionalProperties": false
            },
            "peripheralsConfig": {
              "description": "Optional. Peripherals configuration that workload has access to.",
              "type": "object",
              "properties": {
                "metastoreService": {
                  "description": "Optional. Resource name of an existing Dataproc Metastore service.\n\n Example:\n\n * `projects/[project_id]/locations/[region]/services/[service_id]`",
                  "type": "string"
                },
                "sparkHistoryServerConfig": {
                  "description": "Optional. The Spark History Server configuration for the workload.",
                  "type": "object",
                  "properties": {
                    "dataprocClusterRef": {
                      "description": "Optional. Resource name of an existing Dataproc Cluster to act as a Spark\n History Server for the workload.\n\n Example:\n\n * `projects/[project_id]/regions/[region]/clusters/[cluster_name]`",
                      "type": "object",
                      "oneOf": [
                        {
                          "required": [
                            "name"
                          ],
                          "not": {
                            "required": [
                              "external"
                            ]
                          }
                        },
                        {
                          "required": [
                            "external"
                          ],
                          "not": {
                            "anyOf": [
                              {
                                "required": [
                                  "name"
                                ]
                              },
                              {
                                "required": [
                                  "namespace"
                                ]
                              }
                            ]
                          }
                        }
                      ],
                      "properties": {
                        "external": {
                          "description": "A reference to an externally managed DataprocCluster resource. Should be in the format \"projects/{{projectID}}/regions/{{region}}/clusters/{{clusterName}}\".",
                          "type": "string"
                        },
                        "name": {
                          "description": "The name of a DataprocCluster resource.",
                          "type": "string"
                        },
                        "namespace": {
                          "description": "The namespace of a DataprocCluster resource.",
                          "type": "string"
                        }
                      },
                      "additionalProperties": false
                    }
                  },
                  "additionalProperties": false
                }
              },
              "additionalProperties": false
            }
          },
          "additionalProperties": false
        },
        "labels": {
          "description": "Optional. The labels to associate with this batch. Label **keys** must contain 1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). Label **values** may be empty, but, if present, must contain 1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch.",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "location": {
          "description": "Required.",
          "type": "string"
        },
        "projectRef": {
          "description": "Required.",
          "type": "object",
          "oneOf": [
            {
              "required": [
                "name"
              ],
              "not": {
                "required": [
                  "external"
                ]
              }
            },
            {
              "required": [
                "external"
              ],
              "not": {
                "anyOf": [
                  {
                    "required": [
                      "name"
                    ]
                  },
                  {
                    "required": [
                      "namespace"
                    ]
                  }
                ]
              }
            }
          ],
          "properties": {
            "external": {
              "description": "The `projectID` field of a project, when not managed by Config Connector.",
              "type": "string"
            },
            "kind": {
              "description": "The kind of the Project resource; optional but must be `Project` if provided.",
              "type": "string"
            },
            "name": {
              "description": "The `name` field of a `Project` resource.",
              "type": "string"
            },
            "namespace": {
              "description": "The `namespace` field of a `Project` resource.",
              "type": "string"
            }
          },
          "additionalProperties": false
        },
        "pysparkBatch": {
          "description": "Optional. PySpark batch config.",
          "type": "object",
          "properties": {
            "archiveURIs": {
              "description": "Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "args": {
              "description": "Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "fileURIs": {
              "description": "Optional. HCFS URIs of files to be placed in the working directory of each executor.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "jarFileURIs": {
              "description": "Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "mainPythonFileURI": {
              "description": "Required. The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.",
              "type": "string"
            },
            "pythonFileURIs": {
              "description": "Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: `.py`, `.egg`, and `.zip`.",
              "type": "array",
              "items": {
                "type": "string"
              }
            }
          },
          "additionalProperties": false
        },
        "resourceID": {
          "description": "The DataprocBatch name. If not given, the metadata.name will be used.",
          "type": "string"
        },
        "runtimeConfig": {
          "description": "Optional. Runtime configuration for the batch execution.",
          "type": "object",
          "properties": {
            "autotuningConfig": {
              "description": "Optional. Autotuning configuration of the workload.",
              "type": "object",
              "properties": {
                "scenarios": {
                  "description": "Optional. Scenarios for which tunings are applied.",
                  "type": "array",
                  "items": {
                    "type": "string"
                  }
                }
              },
              "additionalProperties": false
            },
            "cohort": {
              "description": "Optional. Cohort identifier. Identifies families of the workloads having the same shape, e.g. daily ETL jobs.",
              "type": "string"
            },
            "containerImage": {
              "description": "Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.",
              "type": "string"
            },
            "properties": {
              "description": "Optional. A mapping of property names to values, which are used to configure workload execution.",
              "type": "object",
              "additionalProperties": {
                "type": "string"
              }
            },
            "repositoryConfig": {
              "description": "Optional. Dependency repository configuration.",
              "type": "object",
              "properties": {
                "pypiRepositoryConfig": {
                  "description": "Optional. Configuration for PyPi repository.",
                  "type": "object",
                  "properties": {
                    "pypiRepository": {
                      "description": "Optional. PyPi repository address",
                      "type": "string"
                    }
                  },
                  "additionalProperties": false
                }
              },
              "additionalProperties": false
            },
            "version": {
              "description": "Optional. Version of the batch runtime.",
              "type": "string"
            }
          },
          "additionalProperties": false
        },
        "sparkBatch": {
          "description": "Optional. Spark batch config.",
          "type": "object",
          "properties": {
            "archiveURIs": {
              "description": "Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "args": {
              "description": "Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "fileURIs": {
              "description": "Optional. HCFS URIs of files to be placed in the working directory of each executor.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "jarFileURIs": {
              "description": "Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "mainClass": {
              "description": "Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in `jar_file_uris`.",
              "type": "string"
            },
            "mainJarFileURI": {
              "description": "Optional. The HCFS URI of the jar file that contains the main class.",
              "type": "string"
            }
          },
          "additionalProperties": false
        },
        "sparkRBatch": {
          "description": "Optional. SparkR batch config.",
          "type": "object",
          "properties": {
            "archiveURIs": {
              "description": "Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "args": {
              "description": "Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "fileURIs": {
              "description": "Optional. HCFS URIs of files to be placed in the working directory of each executor.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "mainRFileURI": {
              "description": "Required. The HCFS URI of the main R file to use as the driver. Must be a `.R` or `.r` file.",
              "type": "string"
            }
          },
          "additionalProperties": false
        },
        "sparkSQLBatch": {
          "description": "Optional. SparkSql batch config.",
          "type": "object",
          "properties": {
            "jarFileURIs": {
              "description": "Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "queryFileURI": {
              "description": "Required. The HCFS URI of the script that contains Spark SQL queries to execute.",
              "type": "string"
            },
            "queryVariables": {
              "description": "Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name=\"value\";`).",
              "type": "object",
              "additionalProperties": {
                "type": "string"
              }
            }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    },
    "status": {
      "description": "DataprocBatchStatus defines the config connector machine state of DataprocBatch",
      "type": "object",
      "properties": {
        "conditions": {
          "description": "Conditions represent the latest available observations of the object's current state.",
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "lastTransitionTime": {
                "description": "Last time the condition transitioned from one status to another.",
                "type": "string"
              },
              "message": {
                "description": "Human-readable message indicating details about last transition.",
                "type": "string"
              },
              "reason": {
                "description": "Unique, one-word, CamelCase reason for the condition's last transition.",
                "type": "string"
              },
              "status": {
                "description": "Status is the status of the condition. Can be True, False, Unknown.",
                "type": "string"
              },
              "type": {
                "description": "Type is the type of the condition.",
                "type": "string"
              }
            },
            "additionalProperties": false
          }
        },
        "externalRef": {
          "description": "A unique specifier for the DataprocBatch resource in GCP.",
          "type": "string"
        },
        "observedGeneration": {
          "description": "ObservedGeneration is the generation of the resource that was most recently observed by the Config Connector controller. If this is equal to metadata.generation, then that means that the current reported status reflects the most recent desired state of the resource.",
          "type": "integer",
          "format": "int64"
        },
        "observedState": {
          "description": "ObservedState is the state of the resource as most recently observed in GCP.",
          "type": "object",
          "properties": {
            "createTime": {
              "description": "Output only. The time when the batch was created.",
              "type": "string"
            },
            "creator": {
              "description": "Output only. The email address of the user who created the batch.",
              "type": "string"
            },
            "operation": {
              "description": "Output only. The resource name of the operation associated with this batch.",
              "type": "string"
            },
            "runtimeInfo": {
              "description": "Output only. Runtime information about batch execution.",
              "type": "object",
              "properties": {
                "approximateUsage": {
                  "description": "Output only. Approximate workload resource usage, calculated when\n the workload completes (see [Dataproc Serverless pricing]\n (https://cloud.google.com/dataproc-serverless/pricing)).\n\n **Note:** This metric calculation may change in the future, for\n example, to capture cumulative workload resource\n consumption during workload execution (see the\n [Dataproc Serverless release notes]\n (https://cloud.google.com/dataproc-serverless/docs/release-notes)\n for announcements, changes, fixes\n and other Dataproc developments).",
                  "type": "object",
                  "properties": {
                    "acceleratorType": {
                      "description": "Optional. Accelerator type being used, if any",
                      "type": "string"
                    },
                    "milliAcceleratorSeconds": {
                      "description": "Optional. Accelerator usage in (`milliAccelerator` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).",
                      "type": "integer",
                      "format": "int64"
                    },
                    "milliDcuSeconds": {
                      "description": "Optional. DCU (Dataproc Compute Units) usage in (`milliDCU` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).",
                      "type": "integer",
                      "format": "int64"
                    },
                    "shuffleStorageGBSeconds": {
                      "description": "Optional. Shuffle storage usage in (`GB` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).",
                      "type": "integer",
                      "format": "int64"
                    }
                  },
                  "additionalProperties": false
                },
                "currentUsage": {
                  "description": "Output only. Snapshot of current workload resource usage.",
                  "type": "object",
                  "properties": {
                    "acceleratorType": {
                      "description": "Optional. Accelerator type being used, if any",
                      "type": "string"
                    },
                    "milliAccelerator": {
                      "description": "Optional. Milli (one-thousandth) accelerator. (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))",
                      "type": "integer",
                      "format": "int64"
                    },
                    "milliDcu": {
                      "description": "Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).",
                      "type": "integer",
                      "format": "int64"
                    },
                    "milliDcuPremium": {
                      "description": "Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) charged at premium tier (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).",
                      "type": "integer",
                      "format": "int64"
                    },
                    "shuffleStorageGB": {
                      "description": "Optional. Shuffle Storage in gigabytes (GB). (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))",
                      "type": "integer",
                      "format": "int64"
                    },
                    "shuffleStorageGBPremium": {
                      "description": "Optional. Shuffle Storage in gigabytes (GB) charged at premium tier. (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))",
                      "type": "integer",
                      "format": "int64"
                    },
                    "snapshotTime": {
                      "description": "Optional. The timestamp of the usage snapshot.",
                      "type": "string"
                    }
                  },
                  "additionalProperties": false
                },
                "diagnosticOutputURI": {
                  "description": "Output only. A URI pointing to the location of the diagnostics tarball.",
                  "type": "string"
                },
                "endpoints": {
                  "description": "Output only. Map of remote access endpoints (such as web interfaces and APIs) to their URIs.",
                  "type": "object",
                  "additionalProperties": {
                    "type": "string"
                  }
                },
                "outputURI": {
                  "description": "Output only. A URI pointing to the location of the stdout and stderr of the workload.",
                  "type": "string"
                }
              },
              "additionalProperties": false
            },
            "state": {
              "description": "Output only. The state of the batch.",
              "type": "string"
            },
            "stateHistory": {
              "description": "Output only. Historical state information for the batch.",
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "state": {
                    "description": "Output only. The state of the batch at this point in history.",
                    "type": "string"
                  },
                  "stateMessage": {
                    "description": "Output only. Details about the state at this point in history.",
                    "type": "string"
                  },
                  "stateStartTime": {
                    "description": "Output only. The time when the batch entered the historical state.",
                    "type": "string"
                  }
                },
                "additionalProperties": false
              }
            },
            "stateMessage": {
              "description": "Output only. Batch state details, such as a failure description if the state is `FAILED`.",
              "type": "string"
            },
            "stateTime": {
              "description": "Output only. The time when the batch entered a current state.",
              "type": "string"
            },
            "uuid": {
              "description": "Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.",
              "type": "string"
            }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    }
  }
}
